"c",   3
)
mean(blah$colA)
apples <- data.frame(size = rnorm(100000, 5, 2))
oranges <- data.frame(size = rnorm(5000, 5, 2))
apples$fruit <- 'apple'
oranges$fruit <- 'orange'
bootstrap_apples <- bootstraps(apples , times = 5000)
bootstrap_oranges <- bootstraps(oranges, times = 5000)
library(rsample)
bootstrap_apples <- bootstraps(apples , times = 50)
bootstrap_oranges <- bootstraps(oranges, times = 50)
fun <- function(splits) {
mean(analysis(splits)$size)
}
test_apples <- bootstrap_apples %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
fruit = "apple")
library(tidyverse)
test_apples <- bootstrap_apples %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
fruit = "apple")
test_oranges <- bootstrap_oranges %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
fruit = "orange")
means <- test_apples %>%
rbind(test_oranges)
ggplot(means, aes(avg, fill = fruit)) +
geom_histogram(alpha = 0.5,
bins = 70,
aes(y = ..density..),
position = 'identity')+
theme_minimal()
bootstrap_apples <- bootstraps(apples , times = 1000)
bootstrap_oranges <- bootstraps(oranges, times = 1000)
#Mean calculating function
fun <- function(splits) {
mean(analysis(splits)$size)
}
#Creating a df of bootstrapped means
test_apples <- bootstrap_apples %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
fruit = "apple")
test_oranges <- bootstrap_oranges %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
fruit = "orange")
means <- test_apples %>%
rbind(test_oranges)
ggplot(means, aes(avg, fill = fruit)) +
geom_histogram(alpha = 0.5,
bins = 70,
aes(y = ..density..),
position = 'identity')+
theme_minimal()
ggplot(means, aes(avg, fill = fruit)) +
geom_histogram(alpha = 0.5,
bins = 70,
position = 'identity')+
theme_minimal()
ggplot(means, aes(avg, fill = fruit)) +
geom_histogram(alpha = 0.5,
bins = 70,
aes(y = ..density..),
position = 'identity')+
theme_minimal()
# Load library: -----------------------------------------------------------
library(rsample)
library(tidyverse)
library(patchwork)
#conflicted::conflict_prefer("View", "utils")
#conflicted::conflict_prefer("filter", "dplyr")
# Load data: --------------------------------------------------------------
data <- read_csv(here::here("Data",
"base_file_all_final.csv"))
df <- data %>%
filter(version == "v2016") %>%
group_by(user_id) %>%
count() %>%
ungroup() %>%
mutate(engagement_threshold = if_else(n > 1,
"high", "low")) %>%
right_join(data %>%
filter(version == "v2016"), by = "user_id")
# Bootstrapped version: ---------------------------------------------------
# Creating 10000 bootstraped data sets:
df_high <- df %>% filter(engagement_threshold == "high")
df_low <- df %>% filter(engagement_threshold == "low")
bootstrap_high <- bootstraps(df_high, times = 5000)
bootstrap_low <- bootstraps(df_low, times = 5000)
# How I'm doing -----------------------------------------------------------
#Mean calculating function
fun <- function(splits) {
mean(analysis(splits)$how_im_doing)
}
#Creating a df of bootstrapped means
test <- bootstrap_high %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
group = "high")
test2 <- bootstrap_low %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
group = "low")
means <- test %>%
rbind(test2)
p1 <- ggplot(means, aes(avg, fill = group)) +
geom_histogram(alpha = 0.5,
bins = 70,
aes(y = ..density..),
position = 'identity')+
theme_minimal()
# Nose --------------------------------------------------------------------
#Mean calculating function
fun <- function(splits) {
mean(analysis(splits)$nose)
}
#Creating a df of bootstrapped means
test <- bootstrap_high %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
group = "high")
test2 <- bootstrap_low %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
group = "low")
means <- test %>%
rbind(test2)
p2 <- ggplot(means, aes(avg, fill = group)) +
geom_histogram(alpha = 0.5,
bins = 70,
aes(y = ..density..),
position = 'identity')+
theme_minimal()
# Eyes --------------------------------------------------------------------
#Mean calculating function
fun <- function(splits) {
mean(analysis(splits)$eyes)
}
#Creating a df of bootstrapped means
test <- bootstrap_high %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
group = "high")
test2 <- bootstrap_low %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
group = "low")
means <- test %>%
rbind(test2)
p3 <- ggplot(means, aes(avg, fill = group)) +
geom_histogram(alpha = 0.5,
bins = 70,
aes(y = ..density..),
position = 'identity')+
theme_minimal()
# Breathing --------------------------------------------------------------------
#Mean calculating function
fun <- function(splits) {
mean(analysis(splits)$breathing)
}
#Creating a df of bootstrapped means
test <- bootstrap_high %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
group = "high")
test2 <- bootstrap_low %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
group = "low")
means <- test %>%
rbind(test2)
p4 <- ggplot(means, aes(avg, fill = group)) +
geom_histogram(alpha = 0.5,
bins = 70,
aes(y = ..density..),
position = 'identity')+
theme_minimal()
p1|p2|p3|p4
# Is higher variance for smaller groups just mechanically the case --------
#Making some fake data
apples <- data.frame(size = rnorm(100000, 5, 2))
oranges <- data.frame(size = rnorm(5000, 5, 2))
apples$fruit <- 'apple'
oranges$fruit <- 'orange'
#Bootstrapping
bootstrap_apples <- bootstraps(apples , times = 1000)
bootstrap_oranges <- bootstraps(oranges, times = 1000)
#Mean calculating function
fun <- function(splits) {
mean(analysis(splits)$size)
}
#Creating a df of bootstrapped means
test_apples <- bootstrap_apples %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
fruit = "apple")
test_oranges <- bootstrap_oranges %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
fruit = "orange")
means <- test_apples %>%
rbind(test_oranges)
ggplot(means, aes(avg, fill = fruit)) +
geom_histogram(alpha = 0.5,
bins = 70,
aes(y = ..density..),
position = 'identity')+
theme_minimal()
# Yes
apples <- data.frame(size = runif(100000, 5, 2))
oranges <- data.frame(size = runif(5000, 5, 2))
apples <- data.frame(size = runif(100000, min = 0, max = 5))
oranges <- data.frame(size = runif(5000, min = 0, max = 5))
apples$fruit <- 'apple'
oranges$fruit <- 'orange'
#Bootstrapping
bootstrap_apples <- bootstraps(apples , times = 1000)
bootstrap_oranges <- bootstraps(oranges, times = 1000)
#Mean calculating function
fun <- function(splits) {
mean(analysis(splits)$size)
}
#Creating a df of bootstrapped means
test_apples <- bootstrap_apples %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
fruit = "apple")
test_oranges <- bootstrap_oranges %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
fruit = "orange")
means <- test_apples %>%
rbind(test_oranges)
ggplot(means, aes(avg, fill = fruit)) +
geom_histogram(alpha = 0.5,
bins = 70,
aes(y = ..density..),
position = 'identity')+
theme_minimal()
# Yes
# Is higher variance for smaller groups just mechanically the case --------
#Making some fake data
#normal
apples <- data.frame(size = rnorm(100000, 5, 2))
oranges <- data.frame(size = rnorm(5000, 5, 2))
#uniform (same problem)
#apples <- data.frame(size = runif(100000, min = 0, max = 5))
#
#oranges <- data.frame(size = runif(5000, min = 0, max = 5))
apples$fruit <- 'apple'
oranges$fruit <- 'orange'
#Bootstrapping
bootstrap_apples <- bootstraps(apples , times = 1000)
bootstrap_oranges <- bootstraps(oranges, times = 1000)
#Mean calculating function
fun <- function(splits) {
mean(analysis(splits)$size)
}
#Creating a df of bootstrapped means
test_apples <- bootstrap_apples %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
fruit = "apple")
test_oranges <- bootstrap_oranges %>%
transmute(avg = map(splits, fun),
avg = as.numeric(avg),
fruit = "orange")
means <- test_apples %>%
rbind(test_oranges)
ggplot(means, aes(avg, fill = fruit)) +
geom_histogram(alpha = 0.5,
bins = 70,
aes(y = ..density..),
position = 'identity')+
theme_minimal()
# Yes
ggplot(means, aes(avg, fill = fruit)) +
geom_histogram(alpha = 0.5,
bins = 70,
aes(y = ..density..),
position = 'identity')+
theme_minimal()
library(tidyverse)
library(depmixS4)
library(patchwork)
data <- read_csv(here::here("Data",
"base_file_all_final.csv"))
MarkovMixture <- function(x, D, K,
maxits = 500,
threshold = 1e-5,
verbose = TRUE) {
# Based on
#   http://bariskurt.com/learning-markov-mixtures-with-em-derivationmatlab-code/
# Inputs
#   x: list of sequences
#   D: dimension of state space
#   K: number of mixture components (clusters)
# Optional inputs
#   maxits: maximum number of iterations
#   verbose: plot the log-likelihood
#   threshold: if decrease in log-lik is < threshold, stop
N <- length(x)                                 # number of sequences
S <- array(NA, c(D, D, N),                     # transition counts
dimnames = list(from = NULL,
to = NULL,
sequence = 1:N))
x1 <- array(0, c(D, N))                        # initial states
# Utility functions
count_transitions <- function(seq) {
# Contingency table of transitions
seq <- factor(seq, levels = 1:D)
t(table(head(seq, -1), tail(seq, -1))) # check: should this be transposed or not?
}
runif1 <- function(n) {
# Uniform vector that sums to 1
x <- runif(n)
x / sum(x)
}
log_ <- function(x, threshold = -500, ...) {
# Logarithm that avoids producing -Infs
lx <- log(x, ...)
lx[is.infinite(lx)] <- threshold
lx
}
normalise <- function(x) {
# Scale columns to sum to 1 and return in same layout
if (length(dim(x)) == 3) {
dm <- dim(x)
x <- apply(x, 3, function(y) scale(y, center = FALSE, scale = colSums(y)))
dim(x) <- dm
} else { x <- scale(x, center = FALSE, scale = colSums(x)) }
attr(x, 'scaled:scale') <- NULL
x
}
normalise_exp <- function(x) {
# Safely exponentiate and normalise by column
if (length(dim(x)) > 2) warning('untested in 3-d')
x <- apply(x, 2, function(y) y - max(y))
normalise(exp(x))
}
log_sum_exp <- function(x) max(x) + log(sum(exp(x - max(x))))
# Sufficient statistics
for (n in 1:N) {
x1[x[[n]][1], n] <- 1 # initial state
S[, , n] <- count_transitions(x[[n]])
}
dim(S) <- c(D^2, N)
# Random initialisation of model parameters
alpha <- runif1(K)                                   # prior p(z)
p1 <- replicate(K, runif1(D))                        # initial state distributions
A <- array(replicate(D * K, runif1(D)), c(D, D, K))  # transition distributions
# Posterior of latent variables p(z|x); i.e. membership values
z <- array(NA, c(K, N))
# Log likelihood
LL <- rep(NA, maxits)
for (iter in 1:maxits) {
log_p1 <- log_(p1)
log_A <- log_(A)
dim(log_A) <- c(D^2, K)
log_alpha <- log_(alpha)
# Expectation step
llhood <- (t(log_p1) %*% x1) + (t(log_A) %*% S) + replicate(N, log_alpha)
z <- normalise_exp(llhood)
# Likelihood
LL[iter] <- log_sum_exp(llhood)
# Maximisation step
p1 <- normalise(x1 %*% t(z))
A <- normalise(array(S %*% t(z), c(D, D, K)))
alpha <- rowSums(z) / sum(z)
if (verbose) message('K = ', K, '\t Iteration ', iter, '\t Log-likelihood ', LL[iter])
if (threshold > 0 && iter > 1 && (LL[iter] - LL[iter-1]) < threshold) {
LL <- LL[1:iter]
if (verbose) message('Converged with tolerance ', threshold)
break
} # /if
} # /for
dimnames(A) <- list(to = NULL, from = NULL, model = 1:K)
dimnames(p1) <- list(state = NULL, model = NULL)
dimnames(z) <- list(model = NULL, sequence = NULL)
return(list(alpha = alpha,
p1 = p1,
A = A,
z = z,
LL = LL,
niters = iter,
K = K))
} # /fn
multiStartEM <- function(n_runs, ...) {
# Run the EM algorithm multiple times and return the best run
results <- lapply(1:n_runs, function(r) MarkovMixture(...))
likelihoods <- sapply(results, function(run) tail(run$LL, 1))
best_run <- results[[which.max(likelihoods)]]
return(best_run)
}
output_clusters <- function(d, N = 50, plot = TRUE, hidden = FALSE, weight = NULL) {
df <- data.frame(user_id =  seq_data$user_id,
prob = t(d$z),
cluster = factor(apply(d$z, 2, which.max), levels = 1:d$K))
sample_n_groups <- function(tbl, size, replace = FALSE, weight = NULL) {
# source: https://github.com/hadley/dplyr/issues/361
grps <- tbl %>% groups %>% unlist %>% as.character
keep <- tbl %>% summarise() %>% sample_n(size, replace, weight)
tbl %>% semi_join(keep) %>% group_by_(grps)
}
if(plot) {
if(hidden) {
gg <- df %>%
group_by(user_id) %>%
sample_n_groups(N, weight = weight) %>%
inner_join(hidden_seq_df) %>%
group_by(user_id) %>%
mutate(dayOfStudy = 224 - n():1 + 1) %>%
ggplot() +
aes(dayOfStudy, factor(user_id, levels=unique(user_id[order(cluster, user_id)]))) +
ylab('user_id') + xlab('Day of study') + ggtitle(paste('K =', d$K, 'clusters')) +
geom_point(aes(alpha = factor(state, levels = 3:1), colour = cluster)) +
scale_colour_brewer(palette = 'Set1') +
scale_alpha_discrete(name = 'State')+
theme_minimal()
} else {
gg <- df %>%
inner_join(data %>%
filter(lubridate::year(date) == 2018)) %>%
group_by(user_id) %>%
sample_n_groups(N, weight = weight) %>%
distinct(date, .keep_all = TRUE) %>%
ggplot() +
aes(date, factor(user_id, levels = unique(user_id[order(cluster, user_id)]))) +
geom_point(aes(colour = cluster)) +
scale_colour_brewer(palette = 'Set1') +
ylab('user_id') + ggtitle(paste('K =', d$K, 'clusters'))+
theme_minimal()
}
print(gg)
}
return(df)
}
seq_data <- data %>%
filter(lubridate::year(date) == 2018) %>%
group_by(user_id) %>%
distinct(date)%>%
do(sequence = as.numeric(seq.Date(min(.$date),
as.Date('2018-12-31'),
by = '1 day') %in% .$date) + 1)
dataSequence_concat <- unlist(seq_data$sequence) - 1 # 0 = no data; 1 = data
dataSequence_lengths <- sapply(seq_data$sequence, length)
hmm <- depmix(dataSequence_concat ~ 1,
nstates = 3,
ntimes = dataSequence_lengths,
#need to play aout with these values to make sure the model converges)
trstart = #c(1/2, 1/2, 0,
c(1/2, 3/8, 1/8,
#1/4, 1/2, 1/4,
1/8, 3/4, 1/8,
0, 0, 1), # Disengaged is an absorbing state
respstart = c(0.7, 0.1, 1e-10), # Pr(enter data)
instart = c(1, 0, 0),
family = binomial())
fittedHMM <- fit(hmm)
summary(fittedHMM)
hidden_seq_list <- fittedHMM@posterior$state %>%
split(rep(seq_data$user_id, dataSequence_lengths))
hidden_seq_df <- hidden_seq_list %>%
plyr::ldply(data.frame, .id = 'user_id') %>%
rename('state' = X..i..) %>%
group_by(user_id)
w <- as.numeric(1:269 %in% sample(1:269, 50, FALSE))
EM_hid4 <- multiStartEM(20, hidden_seq_list, D = 3, K = 4)
hid_clusters4 <- output_clusters(EM_hid4, hidden = TRUE, weight = w)
w <- as.numeric(1:269 %in% sample(1:269, 50, FALSE))
EM_hid4 <- multiStartEM(20, hidden_seq_list, D = 3, K = 4)
hid_clusters4 <- output_clusters(EM_hid4, hidden = TRUE, weight = w)
hid_clusters4 <- output_clusters(EM_hid4, hidden = FALSE, weight = w)
hid_clusters4 <- output_clusters(EM_hid4, hidden = TRUE, weight = w)
hid_clusters4
EM_hid4
?depmix
hmm <- depmix(dataSequence_concat ~ 1,
nstates = 3,
ntimes = dataSequence_lengths,
#need to play aout with these values to make sure the model converges)
trstart = #c(1/2, 1/2, 0,
c(1/2, 3/8, 1/8,
#1/4, 1/2, 1/4,
1/8, 3/4, 1/8,
0, 0, 1), # Disengaged is an absorbing state,
prior = c(1/8, 3/4, 1/8),
respstart = c(0.7, 0.1, 1e-10), # Pr(enter data)
instart = c(1, 0, 0),
family = binomial())
hmm <- depmix(dataSequence_concat ~ 1,
nstates = 3,
ntimes = dataSequence_lengths,
#need to play aout with these values to make sure the model converges)
trstart = #c(1/2, 1/2, 0,
c(1/2, 3/8, 1/8,
#1/4, 1/2, 1/4,
1/8, 3/4, 1/8,
0, 0, 1), # Disengaged is an absorbing state,
prior =c(1/2, 3/8, 1/8,
#1/4, 1/2, 1/4,
1/8, 3/4, 1/8,
0, 0, 1),
respstart = c(0.7, 0.1, 1e-10), # Pr(enter data)
instart = c(1, 0, 0),
family = binomial())
hid_clusters4 <- output_clusters(EM_hid4, hidden = TRUE, weight = w)
