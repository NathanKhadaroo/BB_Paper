---
title: "New proposal"
output:
  pdf_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract:

This thesis aims to improve our understanding of issues surrounding data quality in data produced using citizen science methodologies in the social sciences. A systematic review of the literature on bias and data quality found numerous concerns around sampling issues that can arise from biases in patterns of participation and attrition. Furthermore, it emerged that solutions to these problems are not universally applicable. Whilst data quality is a key concern for all citizen science studies, and data quality protocols have been developed to ensure data produced by citizen scientists are of sufficient quality, in some instances such as experience sampling method/ecological momentary assessment (ESM/EMA) approaches which focus on subjective data, these protocols are difficult to apply. Instead, researchers using these approaches have tended to use engagement, typically operationalised as the number of contributions made, as a proxy measurement for the quality of the data provided. However, further study is required to validate this practice, which has several potential issues for the reproducibility of studies using ESM/EMA approaches. Using data from the Britain Breathing app, I seek to provide quantitative evidence on whether data produced by low engagement users is indeed of lesser quality, and quantitative and qualitative evidence on the impact of the choice of method through which engagement is operationalised. Critically, I will test how sensitive differences between engagement groups are to the way engagement is operationalised. These findings will be supplemented by an interview study of participants, which will provide qualitative evidence on the motivations of app users and how they differ by level of engagement, and a survival analysis of determinants of attrition, which will provide quantitative evidence for how factors such as symptom intensity affect motivation and attrition.

# Background:

This thesis aims to improve our understanding of the extent and types of data quality issues in social science data generated using citizen science and crowdsourcing approaches.

Citizen science is an increasingly popular approach to scientific inquiry. This label covers a wide variety of practices that aim to involve non-professional scientists in the practice of process (Crain, Cooper, & Dickinson, 2014). An initial "100 abstracts" review found that concerns about the quality of data produced by citizen scientists were widespread among both practitioners and potential audiences such as policy-makers (Aceves-Bueno, et al. 2017; Basiri, et al. 2019; Elliott & Rosenberg, 2019; Lukyanenko, et al. 2016; Riesch & Potter, 2014).

Seeking to expand upon this finding, I conducted a systematic narrative review of issues of bias and data quality in social science studies that use a citizen science or crowdsourced approach for generating spatial data. One of the findings of this review was that there are several biases associated with unequal participation and attrition. Specifically, there are concerns that citizen scientists are not representative of the broader population in either their demographic characteristics, their geographical location, or their motivations, and that attrition happens in unequal, non-random ways.

One suggestion found in the literature to mitigate some of these problems is the use of experience sampling methods, also known as ecological momentary assessments (ESM/EMA), which have been recommended as a means of reducing bias in data caused by unequal participation within a sample by removing participant subjectivity about when and where to make a report (Solymosi, et al. 2020:19).

ESM/EMA refers to a family of methods "for studying what people do, feel, and think during their daily lives" (Larson & Csikszentmihalyi, 2014:21). These approaches allow the collection of self-reported data about participants' subjective experiences when and where they occur, mitigating the impact of biases arising from retrospective reporting, often referred to as memory bias (Beal & Weiss, 200; Schwarz & Clore, 1983; Scollon, et al. 2009; Yarmey, 1979). In these studies, participants are required to provide responses according to "interval-contingent sampling (participants provide responses at pre-determined times), signal-contingent sampling (participants are notified via beeper, text message, etc. at fixed or random intervals when it is time to provide responses), and event-contingent sampling (participants provide responses upon the occurrence of a pre-determined event)" (Sather, 2014). A further advantage is that, unlike traditional survey methods, these approaches are able to produce data that is geotagged and time-stamped at the time and place in the participants daily routine in which they filled in the assessment, allowing the effects of environmental factors to be explored. Whilst these approaches can be conducted "using a wide variety of media, including paper diaries, electronic diaries, or telephones" they are increasingly associated with smartphone technology (Trull & Ebner-Priemer, 2009:457).

Despite the advantages presented by these approaches and their increasing popularity in the social sciences, a number of concerns exist about their use: Surveys among researchers using citizen science approaches have found that data quality is reported as a major concern for all citizen science projects (Riesch & Potter, 2014), and these concerns are also present in the specific case of citizen science projects which use ESM/EMA approaches (Scollon, et al. 2009).

A number of approaches exist to help mitigate concerns around data quality in citizen science studies, such as the implementation of quality assurance standards and protocols (Minghini, et al. 2017; Fonte, et al. 2017; Samulowska, et al, 2021). However, these approaches are not universally applicable and are often developed in contexts where there is a relatively large amount of information about both participants, their contribution history, and the object or phenomena about which they are collecting data. For example, approaches that leverage redundancy in the data collection or classification approach (Balázs, et al. 2021; Lintott et al. 2008), or approaches where strong priors about the distribution of likely observations enable the flagging of unlikely reports for further investigation (Salganik, 2019; Kelling et al. 2012).

Unfortunately, for some studies, and in particular for studies using an ESM/EMA methodology which tend to focus on generating data about the subjective experiences of participants, these approaches are difficultly applicable. For example, the fact that a given participant in a criminology study reports much lower fear of crime in a given area than other participants might not indicate that they are responding carelessly or erroneously: they may simply be less fearful of being a victim of crime in that area than the average participant, for example because it is close to their home (Solymosi, Bowers, & Fujiyama, 2015: 198, 200; Leitner & Kounadi, 2015: 220).

Despite these issues, the need to assess data quality and to identify and remove low-quality responses remains critical for ESM/EMA studies, both for the credibility of the studies as perceived by decision-makers and other academics (Johnson & Sieber, 2013; Riesch & Potter, 2014; Elliott & Rosenberg, 2019) and as a necessary step for unbiased statistical inferences to be made from the generated data (Huang, 2018).

Given how difficult it is to apply standard data quality protocols to ESM/EMA data, the most common approach used in these situations has been to use the number of reports submitted as a proxy for the quality of the data, on the assumption that quality and quantity are associated, since both the attentiveness with which reports were completed and the number of reports completed are assumed to be at least partially determined by a participant's underlying motivation (Doherty, et al. 2020; Geerharts, 2021). Jaso, et al (2021) note that, for EMA/ESM studies, the "closest "best-practice" standard for cleaning data is the tendency to remove participants who do not meet an a-priori compliance cut off defined by the percent of surveys completed" (3).

However, this approach is not without risks. There are several potential problems with this approach:

-   First, it is not known whether or not low-engagement users do produce lesser quality data (or more conservatively, data of insufficient quality). Specifically in the context of ESM/EMA methods, our understanding of the motivations, levels of carelessness, and quality of the data produced by low-engagement participants remains limited (Eveleigh, et al. 2014., Jaso, et al. 2021., Welling, et al. 2021). In particular, it is often unknown, though widely assumed, that low-engagement participants engage in careless responding at higher rates, and produce data of an insufficient quality to be of use (Geerharts, 2021). If this assumption does not hold, we may be at risk of excluding potentially valuable contributions, unnecessarily undermining the statistical power of inferences made using the data. Critically for the field, low power has been identified as a key factor in the irreproducibility of results (Bishop, 2019).

-   Secondly, Solymosi, et al (2020) outline that a "sampling issue specific to any study design that requires ongoing participation is sample attrition over time" (20). Research has shown that attrition in citizen science studies happens in very unequal matters, with a small number of participants remaining engaged for the duration a given study and providing most of the data, and a much larger number of participants dropping out after submitting a small number of contributions (Haklay, 2013). Furthermore, research into participant behaviours in citizen science has so far focused heavily on the former, heavily engaged users with relatively little known about the behaviours of less engaged participants (Eveleigh, 2014). In the specific case of ESM/EMA methods, researchers have argued that the burden of regular contributions leads to most participants either dropping out or contributing less over time (Santangelo, et al. 2013; Scollon, et al. 2009; Doherty, et al. 2020). By removing low-engagement users, we risk exacerbating the sampling biases which occur from non-random attrition. This is especially concerning as low-engagement participants in studies in related fields have been found to be systematically different from other participants (Perski, et al. 2017; Turner, et al. 2017; Rintala, et al. 2019), and there is emerging evidence that this can also be the case in smartphone-based ESM/EMA studies (Druce, et al. 2017; Pratap, et al. 2020; Kronkvist & Engstrom, 2020).

-   Thirdly, there is a concern that studies with a high number of dropouts and unengaged users are dismissed by researchers, highlighting that researchers' concerns about the impact of low engagement on their studies could be fueling publication bias in studies that use EMA/ESM approaches. Publication bias is "the generally accepted term for what occurs when the research that appears in the published literature (or is otherwise readily available) is systematically unrepresentative of the population of completed studies" (Sutton, 2009). For example, in the field of eHealth research, Eysenbach raises the issue that researchers "tend to gloss over high dropout rates, or not to publish their study results at all, as they see their studies as failures" (2005: 1). This raises the concern that what little knowledge is available about low engagement users may be biased by the fact that studies where low engagement is an important factor may not be published.

-   Finally, the arbitrariness and large amount of researcher degrees of freedom for exclusion criteria can lead to multiple comparison problems, which can further undermine the reproducibility of results (Steegen, et al. 2016; Gelman & Loken, 2013; Simonsohn, Simmons, & Nelson, 2020; Ioannidis, 2008). The way that engagement is understood varies a lot between scholars and fields (as shown in reviews by Perski, et al. 2017 & Yardley, et al. 2016) and critically, the way the concept has been operationalised in studies varies hugely, with a growing variety of approaches being available to scholars (Druce, et al. 2017; Jaso, et al. 2021; Kronkvist and Engstrom, 2020; Pratap, et al. 2020; Sun, et al. 2020; Zhao, et al. 2021). Meta-research in psychology has shown that "flexibility in data collection and analysis allows presenting anything as significant" (Simmons, et al. 2012). Exclusion criteria in particular have been shown to be an important source of researcher degrees of freedom, especially when they are chosen post-hoc (Wicherts, et al. 2016: 5). A popular method to address these concerns has been the use of multiverse-style analyses, which aim to visualize the uncertainty caused by such decisions, are an increasingly popular approach. These approaches "estimate an effect across an entire set of possible specifications, to expose the impact of hidden degrees of freedom and/or obtain robust, less biased estimates of the effect of interest" (Del Giudice, et al 2020). However, Del Giudice, & Gangestad have warned that "if specifications are not truly arbitrary, multiverse-style analyses can produce misleading results, potentially hiding meaningful effects within a mass of poorly justified alternatives" (2021). Critically, some authors have argued that their approach to operationalising engagement is an improvement on existing methods and better captures underlying dynamics, thus making their choice non-arbitrary. For example Druce, et al (2017) argue cutoff approaches overlook the complexity of patterns of engagement, failing to account for the possibility that a participant's engagement may vary over time, and propose a alternative approach using Hidden Markov Models which they argue better capture these dynamics. However, little is known about the extent to which the choice of approach to operationalising engagement effects conclusions, biases, and the size of samples.

Recently, meta-researchers on EMA/ESM methods have begun to propose alternatives to the approach of excluding data from participants; specifically, a number of post-hoc methods have been proposed to detect careless responses in ESM/EMA studies (Jaso, et al. 2021; Welling, et al. 2021; Geeraerts, 2021). However, it remains unknown how these new metrics relate to engagement, and whether the mechanisms through which they are assumed to emerge are genuinely reflective of participants' experiences.

This thesis seeks to provide both quantitative and qualitative evidence to enrich our understanding of the quality of the data produced by low engagement participants, their motivations, and the relationship between engagement and carelessness in EMA/ESM studies. In particular, it seeks to provide evidence of how excluding participants from low-engagement groups can impact the conclusions of studies, introduce or exacerbate biases in the data produced, and reduce the statistical power of statistical tests conducted in citizen science studies by losing potentially valuable data.

Furthermore, it aims to generate evidence for whether different approaches to operationalising engagement and detecting low-quality contributions are capable of achieving similar outcomes in terms of preserving data quality and plausibility, whilst retaining more participants, and minimizing biases.

# The dataset:

This thesis will use data from the Britain Breathing project, this section provides a brief overview of the data.

Britain Breathing is a citizen science project which uses an experience sampling method to collect geolocated time series data on seasonal pollen allergy symptoms, also referred to as allergic rhinitis or hayfever (Vigo, et al. 2018).

The motivation behind the Britain Breathing project was to create data that can provide insights into why the incidence of seasonal allergies has been rising dramatically. One popular hypothesis for this rise is that environmental factors play a significant role. However, the testing of this hypothesis has been limited by the absence of geotagged time-series data on symptom intensity which would allow the linking of symptom data to data on environmental factors.

Users of the app can report symptoms at any time, and also at daily scheduled intervals. When they make a report, they are first asked how they are feeling. If they respond that they are feeling well, no further questions are asked, if they respond otherwise, they are asked further questions about the severity of various symptoms ("nose", "eyes, and "breathing"), which they report on a four point scale ranging from 0, which signifies an absence of symptoms, to 3, which indicates severe symptoms.

Available further information includes the gender of participants, their date of birth, known allergies, whether or not they are taking antihistamine medication, and the date, time, and location of each report submitted.

The data has good geographical coverage, with reports submitted from 95% of all postcode areas in the UK, with the average number of reports per postcode being 167 (idem: 89).

The Britain Breathing data provides an opportunity to answer several research questions surrounding carelessness and engagement in smartphone-based ESM/EMA studies. A unique opportunity presented by this data is that, unlike most studies using an ESM/EMA methodology, the scores being produced by participants have a straightforward causal relationship with environmental factors. Whilst participants are still reporting a subjective experience (symptom intensity and related wellbeing), we have strong priors that these are a reaction to environmental factors, principally the presence of pollen in the atmosphere. We are also able to compare the levels of local symptom intensity as reported by app users to validation data such as local antihistamine prescription data.

# Research questions:

My research questions will be organized around four main areas:

**1) What are the main issues surrounding spatial data generated using citizen science and crowdsourcing methodologies?**

This area focused on identifying the main concerns surrounding the data generated by citizens scientists.

To answer this question, I conducted a systematic narrative review (Siddaway, et al. 2019) of the literature on data quality and bias in studies that used citizen science and crowdsourcing methodologies to generate spatial data about social science issues.

I found that bias and data quality was a major concern, and that spatial data presented further novel challenges for the quality and bias of the data produced.

A number of solutions have been proposed for some of the issues of bias and data quality, including groundtruthing, co-design, varying the choice of platform, statistical modelling of the data-generating mechanism, and the use of data quality assurance protocols. However, these are often not applicable outside of the specific type of project in which they were developed.

I also found that there was little research on the extent to which publication bias is an issue, as well as on threats to reproducibility from undisclosed researchers degrees of freedom, and that there were a number of methodological approaches for which most methods for mitigating bias and assuring data quality were not applicable.

**2) The response patterns of low-engagement users:**

This area looks at how distinct the data provided by low-engagement users is from the data generated by other users. A fundamental requirement for low-engagement users to be justifiably excluded from analyses is that there should be robust reasons to believe they are not part of a study's target population. Furthermore, it is important to establish the difference in response scores as simulation studies in the traditional survey literature have shown that "the confounding effect of IER [insufficient effort responding] stems from the weighted mean difference between attentive and IER participants" (Huang, 2018:3). The differences in mean responses between groups can therefore be used to establish an upper bound on the biasing effect of careless responding. This area has two core research questions:

-   How do the responses of low engagement users differ from other users?

*Methodology:*

The question explores the reported data generated by low-engagement users. Answering this question involves analysing group differences in both direct and indirect measures.

The direct measures are the level of reported well being (recorded on a three point scale), as well as scores given on individual symptoms: eyes, nose, and throat (recorded on a four point scale). Indirect measures include the spatial range and reach of the reports submitted (Solymosi, 2019).

An appropriate statistical test is the Games-Howell post-hoc test, as it does not make strong assumptions about equal group sizes (which we do not expect, given that engagement typically follows a power law, with a small number of participants contributing a large share of all reports), or equal variance within groups (which we might not expect, due to variance's potential association with carelessness (Jaso, et al. 2021; Geerhart, 2021)).

-   Do low-engagement users differ from other groups on indicators of data quality?

*Methodology:*

This question explores the widely held assumption that low-engagement users produce less reliable data. It will be approached quantitatively in two ways:

A first approach is to look at how well the responses of low engagement users track other local indicators of symptom intensity, specifically the number of antihistamines prescribed by local general practitioners (which is used as a validation variable by the core BB team (Vigo, et al. 2018)), and local pollen levels. One way of doing so is to calculate correlations between these variables, normalise these correlations using the Fisher-z transformation, and then use the normal distribution to test for differences.

A second approach will be to compare the performance of different engagement groups on recently developed post-hoc indicators of carelessness. Three metrics identified in the literature are applicable, I also propose a fourth novel metric which leverages the spatial nature of the data.

The first metric is referred to as "longstring" detection, and relies upon on the variance of provided responses, operationalised in Jaso, et al (2021) as the proportion of items at the mode, and by Geerhart (2021) as the "longest sequence of repeated responses on consecutive items, divided by the longest possible string".

The second metric, recommended by Welling (2021), looks at the number of further questions triggered by the respondents answers. In the Britain Breathing app, participants who report that they are feeling fine are asked no further questions. In some version of the app, participants were still asked further questions when they answered they were feeling fine, which can allow us to disentangle differences due to the indicator from expected differences in responses between groups.

The third metric looks at correlations between antonyms. This is an established practice for detecting careless responses in the survey literature (eg: Meade & Craig, 2012), it has also been used in every paper examining careless responses in EMA/ESM studies (Jaso, et al 2021; Welling, et al. 2021; and Geeraerts 2021). Geeraerts (2021) measures content consistency via "*synonym and antonym pair violation counts"* whilst Jaso, et al (2021) look at the responses to psychometric antonyms "anxious" and "relaxed", and explore graphically how these relate to other indicators of carelessness such as time to complete per item. In the Britain Breathing app, respondents are first asked to report how they are feeling ("Good", "so-so", and "bad"). If the participant reports that they are feeling good, no further questions were asked. However, for one year of data, an issue with the application meant that all participants were asked all questions. I can therefore look at the correlation between how well they say they are, and how they report their symptoms.

None of the existing approaches found in the literature take advantage of opportunities provided by the spatial nature of ESM/EMA data. With allergic rhinitis symptom data, we have very strong priors that environmental factors explain a large amount of the variation in symptoms over time. It may therefore be useful to explore graphically, in the style of Jaso, et al's approach, how measures such as deviation from local trajectories in symptoms (for example looking at individual-level rates of changes in variance compared to the average level in an area) relate to other indicators of carelessness. Geeraerts suggests using Mahalanobis distance and robust PCA orthogonal distance as measures of outlier status (2021:11).

In addition to these quantitative approaches, qualitative data from the interview study will help further ground and contextualize findings.

**3) The characteristics of low-engagement users:**

This area looks at the characteristics of low-engagement users. Understanding to what extent, if any, low-engagement participants are systematically different from other users is critical for understanding the potential impact of attrition bias and the use of engagement as exclusion criteria.

-   Do low engagement users differ from other users on background information?

*Methodology:*

A number of covariates of engagement have been identified in related fields, in particular age, gender, levels of education, whether the participant has the clinical condition of interest, and ethnicity (Perski, et al. 2017: Druce, et al. 2017; Kronkvist and Engstrom. 2020; Rintala, et al. 2019; Turner, et al. 2017).

Prior to accessing the functionality of the application, participants in the Britain Breathing asks participants for their, age, gender, and whether they suffer from any allergies (though this last question is optional), which will allow these characteristics to be formally compared between groups.

As above, Games-Howell post-hoc tests are an appropriate approach to testing group differences.

Further comparisons are possible by linking data to local data such as census data.

**4) What is the impact of the way we operationalise engagement in smartphone-based ESM/EMA research:**

This area looks at the ways in which engagement has been operationalised in the ESM/EMA literature, explores the impact of these choices, and tests whether certain approaches are more arbitrary than others. Answering this question is crucial to identify the amount of uncertainty resulting from researcher degrees of freedom in analyses using citizen science data which excludes low-engagement participants, and the extend to which those degrees of freedom are arbitrary (Del Giudice & Gangestad, 2021).

-   How much do differences in reporting behaviours and characteristics between engagement groups vary depending on to the way in which engagement has been operationalised?

*Methodology:*

This question explores the impact on group differences in responses, characteristics, and data quality, of using one approach to operationalising engagement over another.

Prior to the analyses in previous steps, I will cluster participants in the Britain Breathing project according to several schemes found in the literature. This will produce a total of 8 classifications, I will then explore how sensitive results are to the way engagement has been operationalised.

The first type of classification is using thresholds to define engagement, this approach is the most commonly used in the literature (Jaso,et al. 2021). In the spirit of sensitivity, I will use a number of arbitrary threshold to label certain users as unengaged: \< 5, \< 10, \< 15, \< 20, and \< 50.

The second type of classification uses data driven thresholds, I will follow the approach taken by Kronkvist and Engstrom (2020) who split their participants into *abstainers*, who completed zero assessments, *dedicated participants* who completed a number of assessments one standard deviation or more above the average number of assessments completed by participants, and *occasional participants* who did not meet the criteria for the two previous groups.

The third type follows Druce, et al (2017), who propose using first-order Hidden Markov Models (HMM). They assume that the observed patterns of contributions are explained by three latent states of engagements:

high engagement -\> high probability of submitting data, low probability of changing state.

low engagement -\> low probability of submitting data, low probability of changing state.

disengaged -\> zero probability of submitting data, zero probability of changing state.

The latter state, where there is zero probability of changing state, is called an absorbing state and is assigned to users who no longer contribute at all. After assigning these states to participants daily behaviors they use mixture models to cluster participants into groups, which they call tourists, low engagement, moderate engagement, and high engagement.

After having assigned the various clusterings, I will conduct the analyses of group differences from questions 1-3, and examine the extent to which the results change, dependent on the way engagement has been operationalised.

The following question explores whether certain approaches map onto underlying differences between groups, regardless of impact.

-   Which, if any, of the approaches to operationalising engagement best maps onto underlying differences in data quality/carelessness, and motivations?

*Methodology:*

This question examines whether the choice of engagement is arbitrary or not, regardless of the impact on results (Del Giudice & Gangestad, 2021). I will explore this with both quantitative, and qualitative data:

-   Using quantitative data, I will examine whether certain approaches to low engagement are more closely associated with data quality. This will be done using carelessness indicators, and by examining whether some low engagement groups are less able than other to track validation variables (such as the local number of prescriptions). Where this is the case, it would suggest that the practice of excluding unengaged users can be valid according to this approach to operationalising it.

-   Using qualitative data from the interview study (more details in the following section), I will asses whether participants who describe themselves as struggling with motivation are more present in certain low engagement groups than others. More generally, once the interviews have been coding and transcribed, I will explore whether certain commonalities are seen across the responses of low engagement users. And whether this depends on the way engagement has been operationalised.

**5) The experiences of low-engagement users:**

This area looks at the experiences and motivations of low-engagement users. It focuses on the qualitative data which is able to provide in depth information about the motivations of participants. However, it also draws upon a survival analysis approach to explore whether provided data, including an improvement in reported symptoms, predicts a drop in engagement.

-   How do low engagement users describe their experiences and motivations using a smartphone-based ESM project?

-   What reasons do they give for not engaging more, and what strategies does their experience suggest to improve participation for low engagement groups?

*Methodology:*

The main way in which this question will be answered is using qualitative data from an interview study of participants. The interviews will be conducted using an open-ended, semi-structured approach. This will allow for a variety of perspectives on a consistent set of questions, whilst allowing interviewees to expand as they wish, potentially unveiling perspectives which were not anticipated (Adams, 2015). One motivation for this approach is a study by Rotman, et al (2012) who find that, when their assumptions are shared with participants, researchers in citizen science studies are often found to be mistaken about the motivations underlying participants decisions to persevere with a study.

Once ethical approval is obtained a request for research volunteers will be published on StudentNet, fliers will also be posted in the appropriate areas around the university. Further recruitment will be solicited via social media, the appropriate mailing lists, through the Manchester data community, and via the Britain Breathing twitter account.

Questions will be open ended, and participants will be prompted to elaborate on elements of their responses. Questions, inspired by the literature (Csikszentmihalyi and Larson, 2014; Jaso, et al. 2021) will include "Do you feel like the data from the application captures your experience well?", "Where there times when you felt less motivated to fill in reports?", "If you stopped using the application for a while or altogether, why did you do so? Was it because your symptoms subsided?".

-   Do participants' experiences of allergic rhinitis symptoms predict disengagement?

This question looks at whether participants' experiences of allergic rhinitis symptoms are an important factor in their engagement. Answering this question involves testing whether a drop in a participant's reported symptom intensity is a meaningful predictor of disengagement. In a study on several health-related data sets, Pratap, et al (2020:1) suggest a number of indicators of retention, two of which are applicable to the Britain Breathing data: having the clinical condition of interest in the study (which was associated with an increase of 7 days compared with controls) and being of older age (which was associated with an increase of 4 days).

Eysenback (2005) highlights that "usage metrics and determinants of attrition should be highlighted, measured, analyzed, and discussed" (1), arguing that "methods of analyzing attrition curves can be drawn from survival analysis methods" (idem). An appropriate model here would be proportional hazards model. Unlike the data studied by Pratac, et al (2020) allergic rhinitis is seasonal. Having the condition of interest is therefore not fixed in time. Fortunately, it is possible to extend the proportional hazards model so as to allow for time-dependent coefficients (Fischer, 1999; Therneau, 2021).

# Timeline:

*So far:*

-   A preliminary review of issues of bias and data quality in citizen science inspired by the "100 abstracts" approach.

-   A systematic review of issues of bias and data quality in social science studies using citizen science and crowdsourced spatial data.

-   A review of the literature on carelessness and engagement in ESM/EMA methods.

-   Learning how to fit probabilistic hidden Markov models and infinite mixture models, as well as a variety of clustering and outlier detection techniques found in the literature.

-   Preparing code for the engagement rankings and group comparisons.

*Project 2 (quantitative elements):*

-   Familiarize myself with the data by the end of January.

-   Coding and fitting models (a lot of the code is done already).

-   Initial discussion of and results findings mid-February.

-   Write up and visualization with the aim to have a first impression by early March.

-   Seeking peer-review and refining based on feedback (possibly via resquant and the cmi group).

*Project 3 (qualitative elements):*

-   Finalize outline and interview questions by early 2022.

-   Discussing with research IT which changes, if any, need to be made to the app. In particular making sure I have a means (even a "hacky" solution) to ensure we can identify the data generated by participants in the follow-up group by the end of January.

-   Passing ethical approval asap.

-   Recruiting participants to join by early/mid-hayfever season (March/April)

-   Interviews with participants at various points throughout the season (up to September, depending on how the season is defined).

-   Attend the "Qualitative methods for quantitative researchers" Summer School.

-   Transcribing, coding, and interpreting interviews (Summer 2022)

-   Comparing how interviewed participants differ from non-interviewed particpants (Autumn 2022)

*Write up:*

-   Writing up the results from the above projects, and ensuring that the three elements flow cohesively.

-   Seeking peer review from PhD groups, postdoc groups, and more formal processes such as journals.

# Ethics:

This section outlines ethical considerations of the proposed thesis.

The first contribution is a review of the literature, which presents no significant ethical constraints.

The second and third parts of the thesis both use data from the Britain Breathing app.

This contains data on symptoms on seasonal allergic rhinitis (hayfever). Whilst this is data is not particularly sensitive, it remains health-related data, and therefore must be treated with caution.

Users of the app are informed before participation that their data will be used for academic research, and have explicitly consented for their data to be used in this way.

The data does not contain direct identifiers, however reports are geographically tagged, and contain variables on year of birth, and gender, which could be used to potentially identify participants. Identification risk from geographical information is mitigated through geographic aggregation at the postcode level.

The data controller is my PhD thesis co-supervisor Dr Caroline Jay, who has been involved in the design of the methodology and has authorized my use of the data for this project.

No identifying information will be published. Model outputs and summary statistics should suffice to communicate the outcome of the research.

The data is stored securely in the University of Manchester's Research Data Storage (Isilon).

The last component of the thesis will involve recruiting participants to use the app and be interviewed about their patterns of participation.

A full ethics application will be submitted using the universities online ethics review software system.

Participants will be required to provide informed consent prior to participation, and will be free to exit at any point, and for any reason.

Participation in this activity presents no potential risks to the participants. Participants will not be named in subsequent write-ups or material submitted for publication.

I am working with research IT on a way to identify participants in the interviews in the broader database, without compromising commitments to participants' privacy.

*Data management plan*

I am currently putting together a data management plan (id number: 89669)

# Contributions:

This research will contribute to a growing understanding of low-engagement participants, building on existing research on engagement, data quality, and carelessness.

Using data from the Britain Breathing project, it will provide the first empirical evidence for the relationship between low engagement and carelessness, with a detailed exploration of how sensitive this relationship is to the way either concept is operationalised.

ESM/EMA researchers will often use the quantity of data provided by participants as a proxy measure of the quality of the data provided. This is mostly due to the difficulty of applying standard citizen science approaches to data validation in an ESM/EMA context. This will advance the field of ESM/EMA research by providing evidence for the validity of widespread approaches such as the removal of low-engagement users.

Furthermore, this thesis will provide another case study for recently developed approaches for detecting careless responses in ESM/EMA studies, as well as proposing a novel technique for detecting careless behavior which leverages the spatial nature of the data.

A further contribution this thesis will make is to ongoing debates surrounding the way in which engagement is operationalised in ESM/EMA studies. In particular, there has so far been no quantitative evidence for the impact of various approaches to defining engagement in terms of participant characteristics or data quality. Critically for the reproducibility of ESM/EMA studies, this thesis will provide an understanding of the extent to which arbitrary researchers degrees of freedom can emerge from the way engagement is operationalised.

An interview study and a survival analysis of existing will further contribute a qualitative and quantitative understanding of the determinants and motivations of low-engagement users in smartphone-based ESM/EMA studies. The interview study will also allow previous findings to be validated, and provide a more in depth understanding of participants' experiences. This will in turn facilitate discussions of how engagement and careful responses can be facilitated in future studies.

# References:

Aceves-Bueno, E., Adeleye, A. S., Feraud, M., Huang, Y., Tao, M., Yang, Y., & Anderson, S. E. (2017). The accuracy of citizen science data: a quantitative review. *Bulletin of the Ecological Society of America*, *98*(4), 278-290.

Adams, W. C. (2015). Conducting semi-structured interviews. *Handbook of practical program evaluation*, *4*, 492-505.

Aristeidou, M., Scanlon, E., & Sharples, M. (2017). Profiles of engagement in online communities of citizen science participation. *Computers in Human Behavior*, *74*, 246-256.

Aghabozorgi, S., Shirkhorshidi, A. S., & Wah, T. Y. (2015). Time-series clustering--a decade review. Information Systems, 53, 16-38.

Albers, C., & Lakens, D. (2018). When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias. Journal of experimental social psychology, 74, 187-195.

Basiri, A., Haklay, M., Foody, G., & Mooney, P. (2019). Crowdsourced geospatial data quality: Challenges and future directions. *International Journal of Geographical Information Science*, *33*(8), 1588-1593.

Balázs, B., Mooney, P., Nováková, E., Bastin, L., & Arsanjani, J.J (2021). "Data Quality in Citizen Science" in Vohland, K., Land-Zandstra, A., Ceccaroni, L., Lemmens, R., Perelló, J., Ponti, M., ... & Wagenknecht, K. *The science of citizen science*. Springer Nature.

Bastl, K., Kmenta, M., & Berger, U. E. (2018). Defining pollen seasons: background and recommendations. Current allergy and asthma reports, 18(12), 1-10.

Bishop, D. (2019). Rein in the four horsemen of irreproducibility. *Nature*, *568*(7753), 435-436.

Blinder, A. S. (1991). Why are prices sticky? Preliminary results from an interview study. *National Bureau of Economics Research Working Paper Series* Working paperNo. 3646

Crain, R., Cooper, C., & Dickinson, J. L. (2014). Citizen science: a tool for integrating studies of human and natural systems. *Annual Review of Environment and Resources*, *39*, 641-665.

Csikszentmihalyi, M., & Larson, R. (2014). Validity and reliability of the experience-sampling method. In *Flow and the foundations of positive psychology* (pp. 35-54). Springer, Dordrecht.

Chung, H., Walls, T. A., & Park, Y. (2007). A latent transition model with logistic regression. Psychometrika, 72(3), 413-435.

Del Giudice, M., & Gangestad, S. W. (2021). A traveler's guide to the multiverse: Promises, pitfalls, and a framework for the evaluation of analytic decisions. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920954925.

Doherty, K., Balaskas, A., & Doherty, G. (2020). The design of ecological momentary assessment technologies. *Interacting with Computers*, *32*(1), 257-278.

Druce, K. L., McBeth, J., van der Veer, S. N., Selby, D. A., Vidgen, B., Georgatzis, K., ... & Dixon, W. G. (2017). Recruitment and ongoing engagement in a UK smartphone study examining the association between weather and pain: cohort study. JMIR mHealth and uHealth, *5*(11), e168.

Eisele,G.,Vachon,H.,Lat,G.,Kuppens,P.,Houben,M.,Myin-Germeys,I.,& Viechtbauer, W. (2020). The effects of sampling frequency and questionnaire length on perceived burden, compliance, and careless responding in experience sampling data in a student population.

Elliott, K. C., & Rosenberg, J. (2019). Philosophical foundations for citizen science. *Citizen Science: Theory and Practice*, *4*(1).

Eveleigh, A., Jennett, C., Blandford, A., Brohan, P., & Cox, A. L. (2014, April). Designing for dabblers and deterring drop-outs in citizen science. In *Proceedings of the SIGCHI Conference on Human Factors in Computing Systems* (pp. 2985-2994).

Fisher, L. D., & Lin, D. Y. (1999). Time-dependent covariates in the Cox proportional-hazards regression model. *Annual review of public health*, *20*(1), 145-157.

Ghahramani, Z. (2001). An introduction to hidden Markov models and Bayesian networks. In Hidden Markov models: applications in computer vision (pp. 9-41).

Haklay, M. (2013). Citizen science and volunteered geographic information: Overview and typology of participation. *Crowdsourcing geographic knowledge*, 105-122.

Hitczenko, M. (2013). Modeling anchoring effects in sequential Likert scale questions (No. 13-15). Working Papers.

Ioannidis, J. P. (2008). Why most discovered true associations are inflated. *Epidemiology*, 640-648.

Jaeschke, R., Singer, J., & Guyatt, G. H. (1989). Measurement of health status: ascertaining the minimal clinically important difference. Controlled clinical trials, 10(4), 407-415.

Jaso, B. A., Kraus, N. I., & Heller, A. S. (2021). Identification of careless responding in ecological momentary assessment research: From posthoc analyses to real-time data monitoring. Psychological Methods.

Johnson, P. A., & Sieber, R. E. (2013). Situating the adoption of VGI by government. In *Crowdsourcing geographic knowledge* (pp. 65-81). Springer, Dordrecht.

Kelling, S., Gerbracht, J., Fink, D., Lagoze, C., Wong, W. K., Yu, J., ... & Gomes, C. (2012, July). ebird: A human/computer learning network for biodiversity conservation and research. In *Twenty-Fourth IAAI Conference*.

Kronkvist, K., & Engström, A. (2020). Feasibility of gathering momentary and daily assessments of fear of crime using a smartphone application (STUNDA): Methodological considerations and findings from a study among Swedish university students. *Methodological Innovations*, *13*(3), 2059799120980306.

Lantz, B. (2013). Equidistance of Likert‑Type Scales and Validation of Inferential Methods Using Experiments and Simulations. Electronic Journal of Business Research Methods, 11(1), pp16-28.

Leiner,D.J.(2019). Too Fast, too Straight, too Weird:Non-Reactive Indicators for Meaningless Data in Internet Surveys. In Survey Research Methods (Vol. 13, No. 3, pp. 229-248).

Leitner, M., & Kounadi, O. (2015). Mapping fear of crime as a context‐dependent everyday experience that varies in space and time. *Legal and Criminological Psychology*, *20*(2), 218-221.

Lintott, C. J., Schawinski, K., Slosar, A., Land, K., Bamford, S., Thomas, D., ... & Vandenberg, J. (2008). Galaxy Zoo: morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey. *Monthly Notices of the Royal Astronomical Society*, *389*(3), 1179-1189.

Lukyanenko, R., Parsons, J., & Wiersma, Y. F. (2016). Emerging problems of data quality in citizen science. *Conservation Biology*, *30*(3), 447-449.

Perski, O., Blandford, A., West, R., & Michie, S. (2017). Conceptualising engagement with digital behaviour change interventions: a systematic review using principles from critical interpretive synthesis. *Translational behavioural medicine*, *7*(2), 254-267.

Pocewicz, A., Nielsen‐Pincus, M., Brown, G., & Schnitzer, R. (2012). An evaluation of internet versus paper‐based methods for public participation geographic information systems (PPGIS). *Transactions in GIS*, *16*(1), 39-53.

Ponciano, L., & Brasileiro, F. (2015). Finding volunteers' engagement profiles in human computation for citizen science projects. *arXiv preprint arXiv:1501.02134*.

Pratap, A., Neto, E. C., Snyder, P., Stepnowsky, C., Elhadad, N., Grant, D., ... & Omberg, L. (2020). Indicators of retention in remote digital health studies: a cross-study evaluation of 100,000 participants. *NPJ digital medicine*, *3*(1), 1-10.

Reade, S., Spencer, K., Sergeant, J. C., Sperrin, M., Schultz, D. M., Ainsworth, J., ... & Dixon, W. G. (2017). Cloudy with a chance of pain: engagement and subsequent attrition of daily data entry in a smartphone pilot study tracking weather, disease severity, and physical activity in patients with rheumatoid arthritis. JMIR mHealth and uHealth, 5(3), e37.

Riesch, H., & Potter, C. (2014). Citizen science as seen by scientists: Methodological, epistemological and ethical dimensions. *Public understanding of science*, *23*(1), 107-120.

Rintala, A., Wampers, M., Myin-Germeys, I., Viechtbauer, W. (2019). Response compliance and predictors thereof in studies using the experience sampling method. Psychological Assessment, 31(2), 226--235. 

Rotman, D., Preece, J., Hammock, J., et al. (2012). Dynamic changes in motivation in collaborative citizen-science projects. *Proc CSCW* , 217--226.

Santangelo, P. S., Ebner-Priemer, U. W., & Trull, T. J. (2013). Experience sampling methods in clinical psychology.

Scollon C. N, Prieto CK., Diener E. (2009) Experience Sampling: Promises and Pitfalls, Strength and Weaknesses. In: Diener E. (eds) Assessing Well-Being. Social Indicators Research Series, vol 39. Springer, Dordrecht. [\<https://doi.org/10.1007/978-90-481-2354-4_8\>](https://doi.org/10.1007/978-90-481-2354-4_8){.uri}

Scheel, A. M. (2021). Why most psychological research findings are not even wrong. <https://doi.org/10.31234/osf.io/8w2sd>

Siddaway, A. P., Wood, A. M., & Hedges, L. V. (2019). How to do a systematic review: a best practice guide for conducting and reporting narrative reviews, meta-analyses, and meta-syntheses. *Annual review of psychology*, *70*, 747-770.

Silvia, P. J., Kwapil, T. R., Eddington, K. M., & Brown, L. H. (2013). Missed beeps and missing data: Dispositional and situational predictors of nonresponse in experience sampling research. Social Science Computer Review, 31(4), 471-481.

Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. *Psychological science*, *22*(11), 1359-1366.

Simonsohn, U., Simmons, J. P., & Nelson, L. D. (2019). Specification curve: Descriptive and inferential statistics on all reasonable specifications. Available at SSRN 2694998.

Smyth, P. (1997). Clustering sequences with hidden Markov models. In Advances in neural information processing systems(pp. 648-654).

Smyth, P., Heckerman, D., & Jordan, M. I. (1997). Probabilistic independence networks for hidden Markov probability models. Neural computation, 9 (2), 227-269.

Solymosi, R. (2019). Exploring spatial patterns of guardianship through civic technology platforms. Criminal Justice Review, 44(1), 42-59.

Solymosi, R., Bowers, K., & Fujiyama, T. (2015). Mapping fear of crime as a context‐dependent everyday experience that varies in space and time. *Legal and Criminological Psychology*, *20*(2), 193-211.

Solymosi, R., Buil-Gil, D., Vozmediano, L., & Guedes, I. S. (2021). Towards a place-based measure of fear of crime: A systematic review of app-based and crowdsourcing approaches. *Environment and Behavior*, *53*(9), 1013-1044.

Steegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. *Perspectives on Psychological Science*, *11*(5), 702-712.

Sun, J., Rhemtulla, M., & Vazire, S. (2020). Eavesdropping on Missing Data: What Are University Students Doing When They Miss Experience Sampling Reports?. Personality and Social Psychology Bulletin, 0146167220964639.

Sutton, A. J. (2009). Publication bias. In *The handbook of research synthesis and meta-analysis*, *2*, 435-452.

Therneau, T., Crowson, C., & Atkinson, E. (2021). Using time dependent covariates and time dependent coefficients in the cox model. *Survival Vignettes*, *2*, 3. Available online: <https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf>

Trull, T. J., & Ebner-Priemer, U. W. (2009). Using experience sampling methods/ecological momentary assessment (ESM/EMA) in clinical assessment and clinical research: introduction to the special section.

Tseng, Y. T., Kawashima, S., Kobayashi, S., Takeuchi, S., & Nakamura, K. (2020). Forecasting the seasonal pollen index by using a hidden Markov model combining meteorological and biological factors. Science of the Total Environment, 698, 134246.

Turner, C. M., Coffin, P., Santos, D., Huffaker, S., Matheson, T., Euren, J., ... & Santos, G. M. (2017). Race/ethnicity, education, and age are associated with engagement in ecological momentary assessment text messaging among substance-using MSM in San Francisco. *Journal of substance abuse treatment*, *75*, 43-48.

UK Health Security Agency (2020) Interview study: qualitative studies. Available online: <https://www.gov.uk/guidance/interview-study-qualitative-studies>

Welling, J., Fischer, R. L., & Schinkel-Bielefeld, N. (2021, June). Is it Possible to Identify Careless Responses with Post-hoc Analysis in EMA Studies?. In Adjunct Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization (pp. 150-156).

Wicherts, J. M., Veldkamp, C. L., Augusteijn, H. E., Bakker, M., Van Aert, R., & Van Assen, M. A. (2016). Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking. *Frontiers in psychology*, *7*, 1832.

Yardley, L., Spring, B. J., Riper, H., Morrison, L. G., Crane, D. H., Curtis, K., ... & Blandford, A. (2016). Understanding and promoting effective engagement with digital behaviour change interventions. *American journal of preventive medicine*, *51*(5), 833-842.

Yarmey, A. D. (1979). The psychology of eyewitness testimony (pp. 204-05). New York: Free Press.

Yeager, C. M., & Benight, C. C. (2018). If we build it, will they come? Issues of engagement with digital health interventions for trauma recovery. *Mhealth*, *4*.

Zhao, Y., Wei, X., Liu, Y., & Liao, Z. (2021). An OSM Contributors Classification Method Based on WPCA and GMM. In *Journal of Physics: Conference Series* (Vol. 2025, No. 1, p. 012040). IOP Publishing.
