---
title: "Data report"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading packages:

```{r, message = FALSE}
library(tidyverse)
library(lubridate)
library(sf)
library(ggspatial)
library(gganimate)
library(ggstatsplot)
library(patchwork)
```

Loading in the raw Britain Breathing data:

```{r, message = FALSE, warning=FALSE}
df <- read_csv(here::here("Data",
                          "base_file_all_final.csv"))
```

The first thing I looked at was how much data is missing:

```{r, message = FALSE}
df %>%
  map(~sum(is.na(.))) %>%
  as_tibble() %>%
  select_if(colSums(.) != 0)
```

Most of these do not concern the planned analysis, however there are 17084 NA's on the how_im_doing variable, which is an issue.

I thought I would try to (roughly) recreate the following plot from Vigo, et al (2017: 90) which uses this variable, so I could see how this missingness varied over time :

![](images/Screenshot%202022-02-02%20at%2014.19.43.png)

This looks more or less the same for the first version (differences are likely due to how I am binning things):

```{r, message = FALSE}
df %>% 
 # filter(date < dmy("01-11-2016") ) %>% 
  ggplot(aes(x = date,
             fill = as_factor(how_im_doing) %>% fct_rev()))+
  geom_histogram(bins = 250, col = "grey", size = 0.1)+
  scale_x_date(date_breaks = '1 months')+
  scale_y_continuous(breaks = seq(0, 2700, 250))+
  scale_fill_manual(values=c("orange", "yellow","green4" ))+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1))+
  labs(fill = "How I'm doing:", x = "", y = "Count")
```

However when we add in the later versions of the application we can see that there is mostly NA values for this variable in later versions. This is something I will talk with Ann (RSE) about.

```{r, message = FALSE}
df %>% 
 # filter(date < dmy("01-11-2016") ) %>% 
  filter(version == "v2016") %>%
  ggplot(aes(x = date,
             fill = as_factor(how_im_doing) %>% fct_rev()))+
  geom_histogram(bins = 200, col = "grey", size = 0.1)+
  scale_x_date(date_breaks = '4 months')+
  scale_y_continuous(breaks = seq(0, 2700, 250))+
  scale_fill_manual(values=c("orange", "yellow","green4" ))+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1))+
  labs(fill = "How I'm doing:", x = "", y = "Count")
```

The next step I took was to look at the spatial aspects of the data, as well as data on antihistamine prescriptions. The smallest areal units I could find for prescription data was NHS Clinical Commissioning Groups (CCG's).

```{r, message = FALSE, warning=FALSE}
# Reading in the datasets

# Prescription data from https://openprescribing.net/bnf/030401/

presc <- read_csv(here::here("Data",
                             "spending-by-ccg-030401.csv"))

# CCG boundary data from https://geoportal.statistics.gov.uk/datasets/d6acd30ad71f4e14b4de808e58d9bc4c/explore

ccgs  <- st_read(here::here("Data",
                           "Clinical_Commissioning_Groups_(April_2021)_EN_BUC",
                           "Clinical_Commissioning_Groups_(April_2021)_EN_BUC.shp")) %>%
  mutate(CCG21NM = toupper(CCG21NM))

# CCG level population data from https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/clinicalcommissioninggroupmidyearpopulationestimates

ccg_pop <- readxl::read_xlsx(here::here("Data",
                                        "sape23dt6amid2020ccg2021estimatesunformatted.xlsx"),
                             sheet = 4) %>%
  janitor::row_to_names(row_number = 6) %>%
  janitor::clean_names() %>% 
  na.omit()%>%
  mutate(ccg_name = toupper(ccg_name))

# Reading the Britain Breathing data as a spatial dataset and reprojecting to BNG

df_sf <- df %>%
  st_as_sf(coords = c("longitude", "latitude"),
           agr = "constant",
           crs = "WGS84") %>%
  st_transform(crs = 27700)

```

I then did a quick plot of the Britain Breathing data over the CCG boundry data to see how they related:

```{r, message = FALSE, warning=FALSE}
ggplot() +
  geom_sf(data = ccgs,
          alpha = 0.5) +
  geom_sf(data = df_sf,
          alpha = 0.05) +
  theme_minimal()
```

Plotting the reports over the CCG's showed some of the reports are in Scotland, Ireland, and Wales (and even a handful in France), where we do not have prescription data.

I then subsetted the data into a new dataset which only contained reports which can be linked to an English CCG, and then did a quick plot to check it had worked:

```{r , message = FALSE, warning=FALSE}
df_sf_england <- df_sf %>%
  st_intersection(ccgs)

ggplot() +
  geom_sf(data = ccgs,
          alpha = 0.5) +
  geom_sf(data = df_sf_england,
          alpha = 0.05) +
  theme_minimal()
```

I then assigned each report to the CCG in which it was made, linked to population data, and tidied up a little:

```{r}
df_sf_england <- df_sf_england %>% 
  st_join(ccgs, left = FALSE)%>%
  left_join(ccg_pop, by = c("CCG21NM.y" = "ccg_name")) %>%
  select(version,
         all_ages,
         report_id,
         user_id,
         timestamp,
         year_of_birth,
         gender,
         hay_fever,
         how_im_doing,
         taken_medication,
         nose,
         eyes,
         breathing,
         date_time,
         "ccg" = "CCG21NM.y")
```

I then did a quick plot of the number of reports in each CCG over time (of course <https://xkcd.com/1138/> applies here, but the plot was mostly just to get an ideas of coverage):

```{r, message = FALSE, warning=FALSE}
# Counting reports by CCG, andfilling in missing data (area/time combos with no reports)

report_counts <- df_sf_england %>%
  mutate(timestamp = ymd_hms(timestamp),
         month_yr = format_ISO8601(timestamp, precision = "ym")) %>%
  group_by(ccg, month_yr)%>% 
  count() %>%
  as_tibble() %>%
  select(ccg, n, month_yr) %>% 
  complete(ccg, month_yr) %>% 
  left_join(ccgs, by = c("ccg"= "CCG21NM" )) %>% 
  filter(ccg != "NHS VALE ROYAL CCG")

ggplot() +
  geom_sf(data = report_counts$geometry, aes(fill = report_counts$n))+
#  scale_fill_gradient(na.value = "grey")+
  transition_states(report_counts$month_yr, transition_length = 1, state_length = 1) +
  labs(title = "Month: {closest_state}",
       fill = "Number of reports:")+
  theme_minimal()
```

This makes clear that there are many areas which receive no reports over certain months (though this isn't so bad during the critical pollen season months).

The next thing I did was to make a quick plot of the prescription data (using *very crude* rates calculated using estimated 2021 populations) to make sure nothing major had gone wrong on that front:

```{r, message = FALSE, warning=FALSE}

# Linking the datasets and calcluating crude prescription rates:

presc_sf <- presc %>% left_join(ccgs,by = c( "row_name" = "CCG21NM")) %>% 
  left_join(ccg_pop, by = c("row_name" = "ccg_name")) %>% 
  filter(row_name != "NHS VALE ROYAL CCG") %>%
  mutate(date = ymd(date),
         presc_rate = (quantity /as.numeric(all_ages)))%>% 
  st_as_sf()

ggplot() +
  geom_sf(data = presc_sf$geometry, aes(fill = presc_sf$presc_rate))+
  transition_states(presc_sf$date, transition_length = 1, state_length = 1) +
  labs(title = "Month: {closest_state}",
       fill = "Average number of items \n prescribed per resident")+
  theme_minimal()
```

One issue I had here is that it was pretty unclear whether the variable I should be plotting here was "items" or "quantity" (though both seem to behave as expected, i.e waxing and waning with the pollen season). Another issue is that NHS VALE ROYAL CCG appears to have been abolished, yet a small amout of data is still being produced by them (my guess would be one or two practices simply haven't updated their systems?), removing it stops this breaking the code.

The next thing I looked at was how long particpants stayed in the study. I started by plotting the number of days in the study against the number of participants.

```{r, message = FALSE, warning=FALSE}
df %>%
  dplyr::filter(version == "v2016") %>%
  group_by(user_id) %>% 
  filter(row_number()==1) %>%
  arrange(-duration_days)
  
  


df %>%
  group_by(user_id) %>% 
  filter(row_number()==1) %>%
  ggplot(aes(x = duration_days)) +
  geom_histogram(bins = 50)+
  theme_minimal()+
  scale_x_log10(n.breaks = 10)+
  labs(x = "Number of days in the study",
       y = "Number of participants")
```

This shows that the overwhelmingly most common outcome was using the application once (this could actually be quite good news for our research question!).

I was also curious about how many participants reported more than once in a given day.

```{r, message = FALSE, warning=FALSE}
df %>%
  group_by(user_id, date) %>%
  count() %>% 
  filter(n > 1) %>%
  ungroup() %>%
  distinct(user_id)
```

There are 587 participants who do so in entire dataset (out of 5403, i.e \~11%).

I was also interested in the number of days where a given person submits multiple reports:

```{r, message = FALSE, warning=FALSE}
df %>%
#  filter(version == "v1") %>% 
#  filter(version == "v2") %>%
#  filter(version == "v2016") %>%
  group_by(user_id, date) %>%
  count() %>% 
  filter(n > 1)
```

1495 days in entire dataset, 661 in v1, 9 in v2, 410 in v2016.

My hunch was this would be mostly people trying out the application on the day they downloaded it. I created a new variable which calculates how long it had been since a participants first report (+1 so a report on a participants first day would be 1), and then plotted this against the amount of days in which participants made multiple reports.

```{r}
df %>%
  mutate(day_in_study = (1 + (date- date_first)) %>% as.numeric()) %>% 
  group_by(user_id, day_in_study) %>%
  count() %>% 
  filter(n > 1) %>%
  ungroup() %>%
  ggplot(aes(x = day_in_study))+
  geom_histogram()+
  theme_minimal()
```

This does seem to happens less over time ***but*** this is probably just due to less reports in total, so my hunch was likely wrong.

I then started to look at implementing some of the proposed clusterings from the litterature.

Kronkvist and Engstrom (2020) split their participants into abstainers, who completed zero assessments, dedicated participants who completed a number of assessments one standard deviation or more above the average number of assessments completed by participants, and occasional participants who did not meet the criteria for the two previous groups.

```{r, message = FALSE, warning=FALSE}
df %>%
  group_by(user_id) %>%
  count() %>%
  ungroup() %>%
  summarise(mean = mean(n),
            stdev = sd(n))
```

The average number of submission per user is \~7 (6.997224), the standard deviation of reports is \~31 (31.11953). I made a quick bar chart of high vs low engagement users according to this approach.

```{r, message = FALSE, warning=FALSE}
df %>%
  group_by(user_id) %>%
  count() %>%
  ungroup() %>% 
  mutate(engagement_kron = if_else(n > 38, "high", "low")) %>%
  ggplot(aes(x = engagement_kron))+
  geom_bar()+
  theme_minimal()
```

Quick peak at group differences:

```{r, message = FALSE, warning=FALSE}
g1 <- df %>%
  group_by(user_id) %>%
  count() %>%
  ungroup() %>% 
  mutate(engagement_kron = if_else(n > 38, "high", "low")) %>%
  left_join(df, by = c("user_id" = "user_id")) %>%
  ggbetweenstats(x = engagement_kron,
                 y = how_im_doing)+
  labs(x = "engagament")

g2 <- df %>%
  group_by(user_id) %>%
  count() %>%
  ungroup() %>% 
  mutate(engagement_kron = if_else(n > 38, "high", "low")) %>%
  left_join(df, by = c("user_id" = "user_id")) %>%
  ggbetweenstats(x = engagement_kron,
                 y = eyes)+
  labs(x = "engagament")

g3 <- df %>%
  group_by(user_id) %>%
  count() %>%
  ungroup() %>% 
  mutate(engagement_kron = if_else(n > 38, "high", "low")) %>%
  left_join(df, by = c("user_id" = "user_id")) %>%
  ggbetweenstats(x = engagement_kron,
                 y = nose)+
  labs(x = "engagament")

g4 <- df %>%
  group_by(user_id) %>%
  count() %>%
  ungroup() %>% 
  mutate(engagement_kron = if_else(n > 38, "high", "low")) %>%
  left_join(df, by = c("user_id" = "user_id")) %>%
  ggbetweenstats(x = engagement_kron,
                 y = breathing)+
  labs(x = "engagament")

(g1 | g2)/
  (g3 | g4)
```

I then started to look at clustering using thresholds:

```{r, message = FALSE, warning=FALSE}
p1 <- df %>%
  group_by(user_id) %>%
  count() %>%
  ungroup() %>% 
  mutate(engagement_threshold = if_else(n > 1, "high", "low")) %>%
  ggplot(aes(x = engagement_threshold))+
  geom_bar()+
  scale_y_continuous(breaks = seq(0, 5500, 500),
                     limits = c(0, 5500))+
  labs(x = "Engagement (threshold = 1)")+
  theme_minimal()

p2 <- df %>%
  group_by(user_id) %>%
  count() %>%
  ungroup() %>% 
  mutate(engagement_threshold = if_else(n > 5, "high", "low")) %>%
  ggplot(aes(x = engagement_threshold))+
  geom_bar()+
  scale_y_continuous(breaks = seq(0, 5500, 500),
                     limits = c(0, 5500))+
  labs(x = "Engagement (threshold = 5)")+
  theme_minimal()

p3 <- df %>%
  group_by(user_id) %>%
  count() %>%
  ungroup() %>% 
  mutate(engagement_threshold = if_else(n > 10, "high", "low")) %>%
  ggplot(aes(x = engagement_threshold))+
  geom_bar()+
  scale_y_continuous(breaks = seq(0, 5500, 500),
                     limits = c(0, 5500))+
  labs(x = "Engagement (threshold = 10)")+
  theme_minimal()

p4 <- df %>%
  group_by(user_id) %>%
  count() %>%
  ungroup() %>% 
  mutate(engagement_threshold = if_else(n > 20, "high", "low")) %>%
  ggplot(aes(x = engagement_threshold))+
  scale_y_continuous(breaks = seq(0, 5500, 500),
                     limits = c(0, 5500))+
  geom_bar()+
  labs(x = "Engagement (threshold = 20)")+
  theme_minimal()

(p1 | p2 ) /
(p3 | p4)
```

A very rough look at group differences in symptoms for the lowest threshold:

```{r, message = FALSE, warning=FALSE}
g1 <- df %>%
  group_by(user_id) %>%
  count() %>%
  ungroup() %>% 
  mutate(engagement_threshold = if_else(n > 1, "high", "low")) %>%
  left_join(df, by = c("user_id" = "user_id")) %>%
  ggbetweenstats(x = engagement_threshold,
                 y = how_im_doing)+
  labs(x = "engagament")

g2 <- df %>%
  group_by(user_id) %>%
  count() %>%
  ungroup() %>% 
  mutate(engagement_threshold = if_else(n > 1, "high", "low")) %>%
  left_join(df, by = c("user_id" = "user_id")) %>%
  ggbetweenstats(x = engagement_threshold,
                 y = eyes)+
  labs(x = "engagament")

g3 <- df %>%
  group_by(user_id) %>%
  count() %>%
  ungroup() %>% 
  mutate(engagement_threshold = if_else(n > 1, "high", "low")) %>%
  left_join(df, by = c("user_id" = "user_id")) %>%
  ggbetweenstats(x = engagement_threshold,
                 y = nose)+
  labs(x = "engagament")

g4 <- df %>%
  group_by(user_id) %>%
  count() %>%
  ungroup() %>% 
  mutate(engagement_threshold = if_else(n > 1, "high", "low")) %>%
  left_join(df, by = c("user_id" = "user_id")) %>%
  ggbetweenstats(x = engagement_threshold,
                 y = breathing)+
  labs(x = "engagament")

(g1 | g2)/
  (g3 | g4)

```

# Hidden Markov Models

The more nuanced approached, specifically using hidden markov models, requires data in long format. This requires some decisions, especially regarding how to treat participants who report more that once a day, and how to treat time.

One option is to have every participants first contribution count as day one, and record whether or not they reported on subsequent days. Given Druce, et al (2017) define engagement on a daily basis, I will begin by considering only whether or not a participant has contributed in a given day or not (ignoring how often).

One issue here is that every participant has a number of empty days equal to the number of days of the longest contributor (1223), this means that pivoting the data to long will create a colossally wide dataset. Is there a smarter way of doing this? Is this even a problem?

```{r, message = FALSE, warning=FALSE}
reported_df <- df %>% 
  mutate(day_in_study = as.numeric(1 + (date- date_first))) %>%
  arrange(user_id, day_in_study) %>%
  group_by(user_id, day_in_study) %>% 
  slice(1) %>%
  ungroup() %>%
  complete(day_in_study, user_id) %>%
  mutate(reported_today = if_else(is.na(date_time),
                                  1,
                                  2))%>%
  pivot_wider(user_id,
              names_from = day_in_study,
              values_from = reported_today)

# Note to self: need to add in a few days (45) where no-one submitted, maybe using something like names(wide_df) %>% as_tibble()?
```

```{r}
test <- df %>% 
  mutate(day_in_study = as.numeric(1 + (date- date_first))) %>%
  arrange(user_id, day_in_study) %>%
  group_by(user_id, day_in_study) %>% 
  slice(1) %>%
  ungroup() %>%
  complete(day_in_study, user_id) %>%
  mutate(reported_today = if_else(is.na(date_time),
                                  0,
                                  1))
```

```{r}
library(depmixS4)

```

This function implements Markov mixture models via the expectation maximization algorithm:

```{r}
MarkovMixture <- function(x, D, K,
                          maxits = 500,
                          threshold = 1e-5,
                          verbose = TRUE) {
  # Based on
  #   http://bariskurt.com/learning-markov-mixtures-with-em-derivationmatlab-code/
  
  # Inputs
  #   x: list of sequences
  #   D: dimension of state space
  #   K: number of mixture components (clusters)
  
  # Optional inputs
  #   maxits: maximum number of iterations
  #   verbose: plot the log-likelihood
  #   threshold: if decrease in log-lik is < threshold, stop
  
  N <- length(x)                                 # number of sequences
  S <- array(NA, c(D, D, N),                     # transition counts
             dimnames = list(from = NULL,
                             to = NULL,
                             sequence = 1:N))
  x1 <- array(0, c(D, N))                        # initial states
  
  # Utility functions
  count_transitions <- function(seq) {
    # Contingency table of transitions
    seq <- factor(seq, levels = 1:D)
    t(table(head(seq, -1), tail(seq, -1))) # check: should this be transposed or not?
  }
  runif1 <- function(n) {
    # Uniform vector that sums to 1
    x <- runif(n)
    x / sum(x)
  }
  log_ <- function(x, threshold = -500, ...) {
    # Logarithm that avoids producing -Infs
    lx <- log(x, ...)
    lx[is.infinite(lx)] <- threshold
    lx
  }
  normalise <- function(x) {
    # Scale columns to sum to 1 and return in same layout
    if (length(dim(x)) == 3) {
      dm <- dim(x)
      x <- apply(x, 3, function(y) scale(y, center = FALSE, scale = colSums(y)))
      dim(x) <- dm
    } else { x <- scale(x, center = FALSE, scale = colSums(x)) }
    attr(x, 'scaled:scale') <- NULL
    x
  }
  normalise_exp <- function(x) {
    # Safely exponentiate and normalise by column
    if (length(dim(x)) > 2) warning('untested in 3-d')
    x <- apply(x, 2, function(y) y - max(y))
    normalise(exp(x))
  }
  log_sum_exp <- function(x) max(x) + log(sum(exp(x - max(x))))
  
  # Sufficient statistics
  for (n in 1:N) {
    x1[x[[n]][1], n] <- 1 # initial state
    S[, , n] <- count_transitions(x[[n]])
  }
  dim(S) <- c(D^2, N)
  
  # Random initialisation of model parameters
  alpha <- runif1(K)                                   # prior p(z)
  p1 <- replicate(K, runif1(D))                        # initial state distributions
  A <- array(replicate(D * K, runif1(D)), c(D, D, K))  # transition distributions
  
  # Posterior of latent variables p(z|x); i.e. membership values
  z <- array(NA, c(K, N))
  
  # Log likelihood
  LL <- rep(NA, maxits)
  
  for (iter in 1:maxits) {
    
    log_p1 <- log_(p1)
    log_A <- log_(A)
    dim(log_A) <- c(D^2, K)
    log_alpha <- log_(alpha)
    
    # Expectation step
    llhood <- (t(log_p1) %*% x1) + (t(log_A) %*% S) + replicate(N, log_alpha)
    z <- normalise_exp(llhood)
    
    # Likelihood
    LL[iter] <- log_sum_exp(llhood)
    
    # Maximisation step
    p1 <- normalise(x1 %*% t(z))
    A <- normalise(array(S %*% t(z), c(D, D, K)))
    alpha <- rowSums(z) / sum(z)
    
    if (verbose) message('K = ', K, '\t Iteration ', iter, '\t Log-likelihood ', LL[iter])
    if (threshold > 0 && iter > 1 && (LL[iter] - LL[iter-1]) < threshold) {
      LL <- LL[1:iter]
      if (verbose) message('Converged with tolerance ', threshold)
      break
    } # /if
    
  } # /for
  
  dimnames(A) <- list(to = NULL, from = NULL, model = 1:K)
  dimnames(p1) <- list(state = NULL, model = NULL)
  dimnames(z) <- list(model = NULL, sequence = NULL)
  
  return(list(alpha = alpha,
              p1 = p1,
              A = A,
              z = z,
              LL = LL,
              niters = iter,
              K = K))
  
} # /fn
```

```{r}
set.seed(123)


hmm <- depmix(dataSequence_concat ~ 1,
              nstates = 3,
              ntimes = dataSequence_lengths,
              trstart = c(1/2, 3/8, 1/8,
                          1/4, 1/2, 1/4,
                          0, 0, 1), # Disengaged is an absorbing state
              respstart = c(0.7, 0.1, 1e-10), # Pr(enter data)
              instart = c(1, 0, 0),
              family = binomial())

fittedHMM <- fit(hmm)
```

These helper functions assist in choosing the best fit:

```{r}
multiStartEM <- function(n_runs, ...) {
  # Run the EM algorithm multiple times and return the best run
  results <- lapply(1:n_runs, function(r) MarkovMixture(...))
  likelihoods <- sapply(results, function(run) tail(run$LL, 1))
  best_run <- results[[which.max(likelihoods)]]
  return(best_run)
}

output_clusters <- function(d, N = 50, plot = TRUE, hidden = FALSE, weight = NULL) {
  df <- data.frame(user_id =  test2$user_id,
                   prob = t(d$z),
                   cluster = factor(apply(d$z, 2, which.max), levels = 1:d$K))
  
  sample_n_groups <- function(tbl, size, replace = FALSE, weight = NULL) {
    # source: https://github.com/hadley/dplyr/issues/361
    grps <- tbl %>% groups %>% unlist %>% as.character
    keep <- tbl %>% summarise() %>% sample_n(size, replace, weight)
    tbl %>% semi_join(keep) %>% group_by_(grps)
  }
  
  if(plot) {
    if(hidden) {
      gg <- df %>%
        group_by(user_id) %>%
        sample_n_groups(N, weight = weight) %>%
        inner_join(hidden_seq_df) %>%
        group_by(user_id) %>%
        mutate(dayOfStudy = 147 - n():1 + 1) %>%
        ggplot() +
          aes(dayOfStudy, factor(user_id, levels=unique(user_id[order(cluster, user_id)]))) +
          ylab('user_id') + xlab('Day of study') + ggtitle(paste('K =', d$K, 'clusters')) +
          geom_point(aes(alpha = factor(state, levels = 3:1), colour = cluster)) +
          scale_colour_brewer(palette = 'Set1') +
          scale_alpha_discrete(name = 'State')
    } else {
      gg <- df %>%
        inner_join(segments3) %>%
        group_by(user_id) %>%
        sample_n_groups(N, weight = weight) %>%
        distinct(entrydate, .keep_all = TRUE) %>%
        ggplot() +
          aes(entrydate, factor(user_id, levels = unique(user_id[order(cluster, user_id)]))) +
          geom_point(aes(colour = cluster)) +
          scale_colour_brewer(palette = 'Set1') +
          ylab('user_id') + ggtitle(paste('K =', d$K, 'clusters'))
    }
    print(gg)
  }
  return(df)
}
```

```{r}

fitted_model <- fit(HMM)

  
 test <-  unlist(reported_df)
 
 
 test <- reported_df %>% 
   select(-user_id) %>%
   unlist()
 
 test %>% View()
 
 
 test2 <- df %>% 
   filter(version == "v2016") %>%
   group_by(user_id) %>% 
   distinct(date)%>%
  do(sequence = as.numeric(seq.Date(min(.$date),
                            as.Date('2016-11-03'),
                            by = '1 day') %in% .$date) + 1)
 
 
 
dataSequence_concat <- unlist(test2$sequence) - 1 # 0 = no data; 1 = data
dataSequence_lengths <- sapply(test2$sequence, length)

hmm <- depmix(dataSequence_concat ~ 1,
              nstates = 3,
              ntimes = dataSequence_lengths,
              trstart = #c(1/2, 1/2, 0, # can't go directly high -> disengaged
                        c(1/2, 3/8, 1/8, # can do so
                        #  1/4, 1/2, 1/4, 
                        1/6, 2/3, 1/6,
                          0, 0, 1), # Disengaged is an absorbing state
              respstart = c(0.7, 0.1, 1e-10), # Pr(enter data)
              instart = c(1, 0, 0),
              family = binomial())
fittedHMM <- fit(hmm)


summary(fittedHMM)


hidden_seq_list <- fittedHMM@posterior$state %>%
  split(rep(test2$user_id, dataSequence_lengths))

hidden_seq_df <- hidden_seq_list %>%
  plyr::ldply(data.frame, .id = 'user_id') %>%
  rename('state' = X..i..) %>%
  group_by(user_id)


hidden_seq_df %>% tally(state)

hidden_seq_df %>%
  ggplot(aes(x = state)) +
  geom_histogram()

```

```{r}

w <- as.numeric(1:4748 %in% sample(1:4748, 50, FALSE))

EM_hid3 <- multiStartEM(20, hidden_seq_list, D = 3, K = 3)
hid_clusters3 <- output_clusters(EM_hid3, hidden = TRUE, weight = w)
```
